{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of cutoutColor_dataaug_ppo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1_WKdcrI6w3"
      },
      "source": [
        "# Getting started with PPO and ProcGen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7LP1JU3I-d4"
      },
      "source": [
        "Here's a bit of code that should help you get started on your projects.\n",
        "\n",
        "The cell below installs `procgen` and downloads a small `utils.py` script that contains some utility functions. You may want to inspect the file for more details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdpZ4lmFHtD8",
        "outputId": "3c7db232-f086-4d01-80f6-1b6a871534f5"
      },
      "source": [
        "!pip install procgen\n",
        "!wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
        "!wget https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: procgen in /usr/local/lib/python3.6/dist-packages (0.10.4)\n",
            "Requirement already satisfied: filelock<4.0.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (3.0.12)\n",
            "Requirement already satisfied: gym<1.0.0,>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.17.3)\n",
            "Requirement already satisfied: gym3<1.0.0,>=0.3.3 in /usr/local/lib/python3.6/dist-packages (from procgen) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from procgen) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym<1.0.0,>=0.15.0->procgen) (1.5.0)\n",
            "Requirement already satisfied: imageio-ffmpeg<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (0.3.0)\n",
            "Requirement already satisfied: glfw<2.0.0,>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.12.0)\n",
            "Requirement already satisfied: moderngl<6.0.0,>=5.5.4 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (5.6.2)\n",
            "Requirement already satisfied: imageio<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (2.9.0)\n",
            "Requirement already satisfied: cffi<2.0.0,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from gym3<1.0.0,>=0.3.3->procgen) (1.14.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym<1.0.0,>=0.15.0->procgen) (0.16.0)\n",
            "Requirement already satisfied: glcontext<3,>=2 in /usr/local/lib/python3.6/dist-packages (from moderngl<6.0.0,>=5.5.4->gym3<1.0.0,>=0.3.3->procgen) (2.2.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio<3.0.0,>=2.6.0->gym3<1.0.0,>=0.3.3->procgen) (7.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi<2.0.0,>=1.13.0->gym3<1.0.0,>=0.3.3->procgen) (2.20)\n",
            "--2020-11-30 13:43:34--  https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14807 (14K) [text/plain]\n",
            "Saving to: ‘utils.py.1’\n",
            "\n",
            "utils.py.1          100%[===================>]  14.46K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-30 13:43:35 (66.3 MB/s) - ‘utils.py.1’ saved [14807/14807]\n",
            "\n",
            "--2020-11-30 13:43:35--  https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7657 (7.5K) [text/plain]\n",
            "Saving to: ‘TransformLayer.py.1’\n",
            "\n",
            "TransformLayer.py.1 100%[===================>]   7.48K  --.-KB/s    in 0s      \n",
            "\n",
            "2020-11-30 13:43:35 (88.5 MB/s) - ‘TransformLayer.py.1’ saved [7657/7657]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDfs7DVPvlhp"
      },
      "source": [
        "Data aug code from\n",
        "https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhVaMUy7ukKz"
      },
      "source": [
        "'''\n",
        "dataaugs:\n",
        "https://github.com/MishaLaskin/rad/blob/1246bfd6e716669126e12c1f02f393801e1692c1/data_augs.py#L296\n",
        "'''\n",
        "'''\n",
        "paper:\n",
        "https://arxiv.org/pdf/2010.10814.pdf\n",
        "'''\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from TransformLayer import ColorJitterLayer\n",
        "\n",
        "\n",
        "def random_crop(imgs, out=84):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: np.array shape (B,C,H,W)\n",
        "        out: output size (e.g. 84)\n",
        "        returns np.array\n",
        "    \"\"\"\n",
        "    n, c, h, w = imgs.shape\n",
        "    crop_max = h - out + 1\n",
        "    w1 = np.random.randint(0, crop_max, n)\n",
        "    h1 = np.random.randint(0, crop_max, n)\n",
        "    cropped = np.empty((n, c, out, out), dtype=imgs.dtype)\n",
        "    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n",
        "        \n",
        "        cropped[i] = img[:, h11:h11 + out, w11:w11 + out]\n",
        "    return cropped\n",
        "\n",
        "\n",
        "def grayscale(imgs):\n",
        "    # imgs: b x c x h x w\n",
        "    device = imgs.device\n",
        "    b, c, h, w = imgs.shape\n",
        "    frames = c // 3\n",
        "    \n",
        "    imgs = imgs.view([b,frames,3,h,w])\n",
        "    imgs = imgs[:, :, 0, ...] * 0.2989 + imgs[:, :, 1, ...] * 0.587 + imgs[:, :, 2, ...] * 0.114 \n",
        "    \n",
        "    imgs = imgs.type(torch.uint8).float()\n",
        "    # assert len(imgs.shape) == 3, imgs.shape\n",
        "    imgs = imgs[:, :, None, :, :]\n",
        "    imgs = imgs * torch.ones([1, 1, 3, 1, 1], dtype=imgs.dtype).float().to(device) # broadcast tiling\n",
        "    return imgs\n",
        "\n",
        "def random_grayscale(images,p=.3):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: torch.tensor shape (B,C,H,W)\n",
        "        device: cpu or cuda\n",
        "        returns torch.tensor\n",
        "    \"\"\"\n",
        "    device = images.device\n",
        "    in_type = images.type()\n",
        "    images = images * 255.\n",
        "    images = images.type(torch.uint8)\n",
        "    # images: [B, C, H, W]\n",
        "    bs, channels, h, w = images.shape\n",
        "    images = images.to(device)\n",
        "    gray_images = grayscale(images)\n",
        "    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n",
        "    mask = rnd <= p\n",
        "    mask = torch.from_numpy(mask)\n",
        "    frames = images.shape[1] // 3\n",
        "    images = images.view(*gray_images.shape)\n",
        "    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n",
        "    mask = mask.type(images.dtype).to(device)\n",
        "    mask = mask[:, :, None, None, None]\n",
        "    out = mask * gray_images + (1 - mask) * images\n",
        "    out = out.view([bs, -1, h, w]).type(in_type) / 255.\n",
        "    return out\n",
        "\n",
        "# random cutout\n",
        "# TODO: should mask this \n",
        "\n",
        "def random_cutout(imgs, min_cut=10,max_cut=30):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: np.array shape (B,C,H,W)\n",
        "        min / max cut: int, min / max size of cutout \n",
        "        returns np.array\n",
        "    \"\"\"\n",
        "\n",
        "    n, c, h, w = imgs.shape\n",
        "    w1 = np.random.randint(min_cut, max_cut, n)\n",
        "    h1 = np.random.randint(min_cut, max_cut, n)\n",
        "    \n",
        "    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n",
        "    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n",
        "        cut_img = img.copy()\n",
        "        cut_img[:, h11:h11 + h11, w11:w11 + w11] = 0\n",
        "        #print(img[:, h11:h11 + h11, w11:w11 + w11].shape)\n",
        "        cutouts[i] = cut_img\n",
        "    return cutouts\n",
        "\n",
        "def random_cutout_color(imgs, min_cut=7,max_cut=22):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: shape (B,C,H,W)\n",
        "        out: output size (e.g. 84)\n",
        "    \"\"\"\n",
        "    \n",
        "    n, c, h, w = imgs.shape\n",
        "    w1 = np.random.randint(min_cut, max_cut, n)\n",
        "    h1 = np.random.randint(min_cut, max_cut, n)\n",
        "    \n",
        "    cutouts = np.empty((n, c, h, w), dtype=imgs.dtype)\n",
        "    rand_box = np.random.randint(0, 255, size=(n, c)) / 255.\n",
        "    for i, (img, w11, h11) in enumerate(zip(imgs, w1, h1)):\n",
        "        cut_img = img.copy()\n",
        "        \n",
        "        # add random box\n",
        "        cut_img[:, h11:h11 + h11, w11:w11 + w11] = np.tile(\n",
        "            rand_box[i].reshape(-1,1,1),                                                \n",
        "            (1,) + cut_img[:, h11:h11 + h11, w11:w11 + w11].shape[1:])\n",
        "        \n",
        "        cutouts[i] = cut_img\n",
        "    return cutouts\n",
        "\n",
        "# random flip\n",
        "\n",
        "def random_flip(images,p=.2):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: torch.tensor shape (B,C,H,W)\n",
        "        device: cpu or gpu, \n",
        "        p: prob of applying aug,\n",
        "        returns torch.tensor\n",
        "    \"\"\"\n",
        "    # images: [B, C, H, W]\n",
        "    device = images.device\n",
        "    bs, channels, h, w = images.shape\n",
        "    \n",
        "    images = images.to(device)\n",
        "\n",
        "    flipped_images = images.flip([3])\n",
        "    \n",
        "    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n",
        "    mask = rnd <= p\n",
        "    mask = torch.from_numpy(mask)\n",
        "    frames = images.shape[1] #// 3\n",
        "    images = images.view(*flipped_images.shape)\n",
        "    mask = mask[:, None] * torch.ones([1, frames]).type(mask.dtype)\n",
        "    \n",
        "    mask = mask.type(images.dtype).to(device)\n",
        "    mask = mask[:, :, None, None]\n",
        "    \n",
        "    out = mask * flipped_images + (1 - mask) * images\n",
        "\n",
        "    out = out.view([bs, -1, h, w])\n",
        "    return out\n",
        "\n",
        "# random rotation\n",
        "\n",
        "def random_rotation(images,p=.3):\n",
        "    \"\"\"\n",
        "        args:\n",
        "        imgs: torch.tensor shape (B,C,H,W)\n",
        "        device: str, cpu or gpu, \n",
        "        p: float, prob of applying aug,\n",
        "        returns torch.tensor\n",
        "    \"\"\"\n",
        "    device = images.device\n",
        "    # images: [B, C, H, W]\n",
        "    bs, channels, h, w = images.shape\n",
        "    \n",
        "    images = images.to(device)\n",
        "\n",
        "    rot90_images = images.rot90(1,[2,3])\n",
        "    rot180_images = images.rot90(2,[2,3])\n",
        "    rot270_images = images.rot90(3,[2,3])    \n",
        "    \n",
        "    rnd = np.random.uniform(0., 1., size=(images.shape[0],))\n",
        "    rnd_rot = np.random.randint(1, 4, size=(images.shape[0],))\n",
        "    mask = rnd <= p\n",
        "    mask = rnd_rot * mask\n",
        "    mask = torch.from_numpy(mask).to(device)\n",
        "    \n",
        "    frames = images.shape[1]\n",
        "    masks = [torch.zeros_like(mask) for _ in range(4)]\n",
        "    for i,m in enumerate(masks):\n",
        "        m[torch.where(mask==i)] = 1\n",
        "        m = m[:, None] * torch.ones([1, frames]).type(mask.dtype).type(images.dtype).to(device)\n",
        "        m = m[:,:,None,None]\n",
        "        masks[i] = m\n",
        "    \n",
        "    \n",
        "    out = masks[0] * images + masks[1] * rot90_images + masks[2] * rot180_images + masks[3] * rot270_images\n",
        "\n",
        "    out = out.view([bs, -1, h, w])\n",
        "    return out\n",
        "\n",
        "\n",
        "# random color\n",
        "\n",
        "    \n",
        "\n",
        "def random_convolution(imgs):\n",
        "    '''\n",
        "    random covolution in \"network randomization\"\n",
        "    \n",
        "    (imbs): B x (C x stack) x H x W, note: imgs should be normalized and torch tensor\n",
        "    '''\n",
        "    _device = imgs.device\n",
        "    \n",
        "    img_h, img_w = imgs.shape[2], imgs.shape[3]\n",
        "    num_stack_channel = imgs.shape[1]\n",
        "    num_batch = imgs.shape[0]\n",
        "    num_trans = num_batch\n",
        "    batch_size = int(num_batch / num_trans)\n",
        "    \n",
        "    # initialize random covolution\n",
        "    rand_conv = nn.Conv2d(3, 3, kernel_size=3, bias=False, padding=1).to(_device)\n",
        "    \n",
        "    for trans_index in range(num_trans):\n",
        "        torch.nn.init.xavier_normal_(rand_conv.weight.data)\n",
        "        temp_imgs = imgs[trans_index*batch_size:(trans_index+1)*batch_size]\n",
        "        temp_imgs = temp_imgs.reshape(-1, 3, img_h, img_w) # (batch x stack, channel, h, w)\n",
        "        rand_out = rand_conv(temp_imgs)\n",
        "        if trans_index == 0:\n",
        "            total_out = rand_out\n",
        "        else:\n",
        "            total_out = torch.cat((total_out, rand_out), 0)\n",
        "    total_out = total_out.reshape(-1, num_stack_channel, img_h, img_w)\n",
        "    return total_out\n",
        "\n",
        "\n",
        "def random_color_jitter(imgs):\n",
        "    \"\"\"\n",
        "        inputs np array outputs tensor\n",
        "    \"\"\"\n",
        "    b,c,h,w = imgs.shape\n",
        "    imgs = imgs.view(-1,3,h,w)\n",
        "    transform_module = nn.Sequential(ColorJitterLayer(brightness=0.4, \n",
        "                                                contrast=0.4,\n",
        "                                                saturation=0.4, \n",
        "                                                hue=0.5, \n",
        "                                                p=1.0, \n",
        "                                                batch_size=b,\n",
        "                                                stack_size=1))\n",
        "\n",
        "    imgs = transform_module(imgs).view(b,c,h,w)\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def random_translate(imgs, size, return_random_idxs=False, h1s=None, w1s=None):\n",
        "    n, c, h, w = imgs.shape\n",
        "    assert size >= h and size >= w\n",
        "    outs = np.zeros((n, c, size, size), dtype=imgs.dtype)\n",
        "    h1s = np.random.randint(0, size - h + 1, n) if h1s is None else h1s\n",
        "    w1s = np.random.randint(0, size - w + 1, n) if w1s is None else w1s\n",
        "    for out, img, h1, w1 in zip(outs, imgs, h1s, w1s):\n",
        "        out[:, h1:h1 + h, w1:w1 + w] = img\n",
        "    if return_random_idxs:  # So can do the same to another set of imgs.\n",
        "        return outs, dict(h1s=h1s, w1s=w1s)\n",
        "    return outs\n",
        "\n",
        "\n",
        "def no_aug(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     import time \n",
        "#     from tabulate import tabulate\n",
        "#     def now():\n",
        "#         return time.time()\n",
        "#     def secs(t):\n",
        "#         s = now() - t\n",
        "#         tot = round((1e5 * s)/60,1)\n",
        "#         return round(s,3),tot\n",
        "\n",
        "#     x = np.load('data_sample.npy',allow_pickle=True)\n",
        "#     x = np.concatenate([x,x,x],1)\n",
        "#     device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#     x = torch.from_numpy(x).to(device)\n",
        "#     x = x.float() / 255.\n",
        "\n",
        "#     # crop\n",
        "#     t = now()\n",
        "#     random_crop(x.cpu().numpy(),64)\n",
        "#     s1,tot1 = secs(t)\n",
        "#     # grayscale \n",
        "#     t = now()\n",
        "#     random_grayscale(x,p=.5)\n",
        "#     s2,tot2 = secs(t)\n",
        "#     # normal cutout \n",
        "#     t = now()\n",
        "#     random_cutout(x.cpu().numpy(),10,30)\n",
        "#     s3,tot3 = secs(t)\n",
        "#     # color cutout \n",
        "#     t = now()\n",
        "#     random_cutout_color(x.cpu().numpy(),10,30)\n",
        "#     s4,tot4 = secs(t)\n",
        "#     # flip \n",
        "#     t = now()\n",
        "#     random_flip(x,p=.5)\n",
        "#     s5,tot5 = secs(t)\n",
        "#     # rotate \n",
        "#     t = now()\n",
        "#     random_rotation(x,p=.5)\n",
        "#     s6,tot6 = secs(t)\n",
        "#     # rand conv \n",
        "#     t = now()\n",
        "#     random_convolution(x)\n",
        "#     s7,tot7 = secs(t)\n",
        "#     # rand color jitter \n",
        "#     t = now()\n",
        "#     random_color_jitter(x)\n",
        "#     s8,tot8 = secs(t)\n",
        "    \n",
        "#     print(tabulate([['Crop', s1,tot1], \n",
        "#                     ['Grayscale', s2,tot2], \n",
        "#                     ['Normal Cutout', s3,tot3], \n",
        "#                     ['Color Cutout', s4,tot4], \n",
        "#                     ['Flip', s5,tot5], \n",
        "#                     ['Rotate', s6,tot6], \n",
        "#                     ['Rand Conv', s7,tot7], \n",
        "#                     ['Color Jitter', s8,tot8]], \n",
        "#                     headers=['Data Aug', 'Time / batch (secs)', 'Time / 100k steps (mins)']))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bn2rkllGJPtZ"
      },
      "source": [
        "Hyperparameters. These values should be a good starting point. You can modify them later once you have a working implementation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Z8P1ehENCwc"
      },
      "source": [
        "# Hyperparameters\n",
        "total_steps = 8e6\n",
        "num_envs = 32\n",
        "num_levels = 100\n",
        "num_steps = 256\n",
        "num_epochs = 3\n",
        "batch_size = 512 #512\n",
        "eps = .2\n",
        "grad_eps = .5\n",
        "value_coef = .5\n",
        "entropy_coef = .01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_p33_G2yfjoC"
      },
      "source": [
        "# !sudo apt-get install imagemagick"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxRWy_T9JY4M"
      },
      "source": [
        "Network definitions. We have defined a policy network for you in advance. It uses the popular `NatureDQN` encoder architecture (see below), while policy and value functions are linear projections from the encodings. There is plenty of opportunity to experiment with architectures, so feel free to do that! Perhaps implement the `Impala` encoder from [this paper](https://arxiv.org/pdf/1802.01561.pdf) (perhaps minus the LSTM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTBV9xpKpEFa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adbf4977-6ff8-4d0e-8a36-5bf1fcec0f09"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from utils import make_env, Storage, orthogonal_init\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels, feature_dim):\n",
        "    super().__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "        Flatten(),\n",
        "        nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "    )\n",
        "    self.apply(orthogonal_init)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.layers(x)\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "  def __init__(self, encoder, feature_dim, num_actions):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "    self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "    self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "  def act(self, x):\n",
        "    with torch.no_grad():\n",
        "      x = x.cuda().contiguous()\n",
        "      dist, value = self.forward(x)\n",
        "      action = dist.sample()\n",
        "      log_prob = dist.log_prob(action)\n",
        "    \n",
        "    return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    logits = self.policy(x)\n",
        "    value = self.value(x).squeeze(1)\n",
        "    dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "    return dist, value\n",
        "\n",
        "\n",
        "# Define environmentbossfight\n",
        "# check the utils.py file for info on arguments\n",
        "env = make_env(num_envs, num_levels=num_levels, env_name='starpilot', use_backgrounds=True)\n",
        "print('Observation space:', env.observation_space)\n",
        "print('Action space:', env.action_space.n)\n",
        "\n",
        "# Define network\n",
        "encoder = Encoder(3,512)\n",
        "policy = Policy(encoder, 512, 15)\n",
        "policy.cuda()\n",
        "\n",
        "# Define optimizer\n",
        "# these are reasonable values but probably not optimal\n",
        "optimizer = torch.optim.Adam(policy.parameters(), lr=5e-4, eps=1e-5)\n",
        "\n",
        "# Define temporary storage\n",
        "# we use this to collect transitions during each iteration\n",
        "storage = Storage(\n",
        "    env.observation_space.shape,\n",
        "    num_steps,\n",
        "    num_envs\n",
        ")\n",
        "\n",
        "''' make separate environment for evaluation '''\n",
        "eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n",
        "eval_obs = eval_env.reset()\n",
        "\n",
        "\n",
        "from collections import deque\n",
        "eval_info_queue=deque(maxlen=num_steps)\n",
        "eval_reward_queue=torch.zeros(num_steps, num_envs)\n",
        "\n",
        "# Run training\n",
        "obs = env.reset()\n",
        "step = 0\n",
        "print(\"NN setup, Training Starts\")\n",
        "while step < total_steps:\n",
        "\n",
        "  policy.eval()\n",
        "  \n",
        "  '''list for storing eval rewards'''\n",
        "  total_reward = []\n",
        "  # Use policy to collect data for num_steps steps\n",
        "  \n",
        "  for _ in range(num_steps):\n",
        "    # Use policy\n",
        "    action, log_prob, value = policy.act(obs)\n",
        "    \n",
        "    # Take step in environment\n",
        "    next_obs, reward, done, info = env.step(action)\n",
        "\n",
        "    # Store data\n",
        "    storage.store(obs, action, reward, done, info, log_prob, value)\n",
        "    \n",
        "    # Update current observation\n",
        "    obs = next_obs\n",
        "\n",
        "    '''evaluate on 512 or numsteps iteartions here'''\n",
        "    eval_action, eval_log_prob, eval_value = policy.act(eval_obs)\n",
        "    eval_obs, eval_reward, eval_done, eval_info = eval_env.step(eval_action)\n",
        "    total_reward.append(torch.Tensor(eval_reward))\n",
        "    ''''''\n",
        "  '''once out of the loop we get the mean validation reward and print it'''\n",
        "  total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "  print(f'Step: {step}\\tEval reward: {total_reward}')\n",
        "  ''''''\n",
        "  # Add the last observation to collected data\n",
        "  _, _, value = policy.act(obs)\n",
        "  storage.store_last(obs, value)\n",
        "\n",
        "  # Compute return and advantage\n",
        "  storage.compute_return_advantage()\n",
        "\n",
        "  # Optimize policy\n",
        "  policy.train()\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    # Iterate over batches of transitions\n",
        "    generator = storage.get_generator(batch_size)\n",
        "    for batch in generator:\n",
        "      b_obs, b_action, b_log_prob, b_value, b_returns, b_advantage = batch\n",
        "\n",
        "      # apply color jitter\n",
        "      # b_obs = b_obs.to('cpu')\n",
        "      'not sure why doing this but resulting image has 9 channels'\n",
        "      # b_obs=np.concatenate([b_obs,b_obs,b_obs], 1)\n",
        "      # b_obs = torch.from_numpy(b_obs).to('cuda')\n",
        "      \n",
        "      # import torchvision\n",
        "      # ss=torch.squeeze(b_obs)\n",
        "      # from google.colab.patches import cv2_imshow\n",
        "      # cv2_imshow(ss.to('cpu').permute(1, 2, 0).numpy())\n",
        "      \n",
        "      b_obs = random_cutout_color(b_obs.to('cpu').numpy())\n",
        "      b_obs=torch.from_numpy(b_obs).to('cuda')\n",
        "      \n",
        "      # Get current policy outputs\n",
        "      new_dist, new_value = policy(b_obs)\n",
        "      new_log_prob = new_dist.log_prob(b_action)\n",
        "      # log_prob\n",
        "      # Clipped policy objective\n",
        "      #print(str(log_prob.shape) + \" \" + str(b_log_prob.shape) + \" \" + str(new_log_prob.shape))\n",
        "      ratio = torch.exp(new_log_prob - b_log_prob)\n",
        "      \n",
        "      clipped_ratio = ratio.clamp(min=1.0 - eps, max=1.0 + eps) \n",
        "      policy_reward = torch.min(ratio * b_advantage, clipped_ratio * b_advantage)\n",
        "      #clip_fraction = (abs((ratio - 1.0)) > clip).to(torch.float).mean()\n",
        "      pi_loss = -policy_reward.mean()\n",
        "\n",
        "      # Clipped value function objective\n",
        "      # clipped_value = new_value + (b_value - new_value).clamp(min=-eps,max=eps)\n",
        "      # vf_loss=torch.max((b_value-b_returns)**2, (clipped_value-b_returns)**2)\n",
        "      # value_loss = 0.5 * vf_loss.mean()\n",
        "\n",
        "      # clipped_value = b_value + (new_value - b_value).clamp(min=-eps,max=eps) #\n",
        "      # vf_loss=torch.max((new_value-b_returns)**2, (clipped_value-b_returns)**2) #\n",
        "      # value_loss = 0.5 * vf_loss.mean() #\n",
        "      clipped_value = (new_value - b_value).clamp(min=-eps,max=eps)\n",
        "      value_loss = 0.5 * torch.max(torch.pow(new_value - b_returns,2), torch.pow(b_value - b_returns, 2)).mean()\n",
        "\n",
        "      # Entropy loss\n",
        "      entropy_loss = new_dist.entropy().mean()\n",
        "\n",
        "      # Backpropagate losses\n",
        "      # loss = torch.mean(pi_loss+value_coef*value_loss+entropy_coef*entropy_loss) #\n",
        "      loss = pi_loss + value_coef * value_loss - entropy_coef * entropy_loss\n",
        "      loss.backward()\n",
        "\n",
        "      # Clip gradients\n",
        "      torch.nn.utils.clip_grad_norm_(policy.parameters(), grad_eps)\n",
        "\n",
        "      # Update policy\n",
        "      optimizer.step()\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "  # Update stats\n",
        "  step += num_envs * num_steps\n",
        "  print(f'Step: {step}\\tMean reward: {storage.get_reward()}')\n",
        "\n",
        "\n",
        "print('Completed training!')\n",
        "torch.save(policy.state_dict(), 'checkpoint.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Observation space: Box(0.0, 1.0, (3, 64, 64), float32)\n",
            "Action space: 15\n",
            "NN setup, Training Starts\n",
            "Step: 0\tEval reward: 7.715677738189697\n",
            "Step: 8192\tMean reward: 5.0625\n",
            "Step: 8192\tEval reward: 5.55593204498291\n",
            "Step: 16384\tMean reward: 4.8125\n",
            "Step: 16384\tEval reward: 7.016532897949219\n",
            "Step: 24576\tMean reward: 5.59375\n",
            "Step: 24576\tEval reward: 6.393898010253906\n",
            "Step: 32768\tMean reward: 7.65625\n",
            "Step: 32768\tEval reward: 5.369840621948242\n",
            "Step: 40960\tMean reward: 6.46875\n",
            "Step: 40960\tEval reward: 6.349653244018555\n",
            "Step: 49152\tMean reward: 6.4375\n",
            "Step: 49152\tEval reward: 5.0744242668151855\n",
            "Step: 57344\tMean reward: 6.15625\n",
            "Step: 57344\tEval reward: 5.498608589172363\n",
            "Step: 65536\tMean reward: 5.84375\n",
            "Step: 65536\tEval reward: 5.844857215881348\n",
            "Step: 73728\tMean reward: 5.53125\n",
            "Step: 73728\tEval reward: 5.540700435638428\n",
            "Step: 81920\tMean reward: 5.78125\n",
            "Step: 81920\tEval reward: 5.078890800476074\n",
            "Step: 90112\tMean reward: 6.9375\n",
            "Step: 90112\tEval reward: 5.576451301574707\n",
            "Step: 98304\tMean reward: 6.15625\n",
            "Step: 98304\tEval reward: 6.082676887512207\n",
            "Step: 106496\tMean reward: 7.0\n",
            "Step: 106496\tEval reward: 6.004238128662109\n",
            "Step: 114688\tMean reward: 7.46875\n",
            "Step: 114688\tEval reward: 6.171463489532471\n",
            "Step: 122880\tMean reward: 6.0\n",
            "Step: 122880\tEval reward: 5.512322902679443\n",
            "Step: 131072\tMean reward: 6.875\n",
            "Step: 131072\tEval reward: 5.421960830688477\n",
            "Step: 139264\tMean reward: 8.28125\n",
            "Step: 139264\tEval reward: 6.04214334487915\n",
            "Step: 147456\tMean reward: 8.6875\n",
            "Step: 147456\tEval reward: 6.010295867919922\n",
            "Step: 155648\tMean reward: 8.46875\n",
            "Step: 155648\tEval reward: 5.197005748748779\n",
            "Step: 163840\tMean reward: 9.90625\n",
            "Step: 163840\tEval reward: 6.0926008224487305\n",
            "Step: 172032\tMean reward: 9.625\n",
            "Step: 172032\tEval reward: 4.932194709777832\n",
            "Step: 180224\tMean reward: 9.15625\n",
            "Step: 180224\tEval reward: 5.7215256690979\n",
            "Step: 188416\tMean reward: 9.0625\n",
            "Step: 188416\tEval reward: 4.593099117279053\n",
            "Step: 196608\tMean reward: 8.5625\n",
            "Step: 196608\tEval reward: 5.871465682983398\n",
            "Step: 204800\tMean reward: 8.0625\n",
            "Step: 204800\tEval reward: 6.193381309509277\n",
            "Step: 212992\tMean reward: 9.4375\n",
            "Step: 212992\tEval reward: 5.566953659057617\n",
            "Step: 221184\tMean reward: 9.21875\n",
            "Step: 221184\tEval reward: 6.1419758796691895\n",
            "Step: 229376\tMean reward: 8.21875\n",
            "Step: 229376\tEval reward: 6.533718585968018\n",
            "Step: 237568\tMean reward: 10.03125\n",
            "Step: 237568\tEval reward: 6.056435585021973\n",
            "Step: 245760\tMean reward: 10.15625\n",
            "Step: 245760\tEval reward: 6.6783647537231445\n",
            "Step: 253952\tMean reward: 9.9375\n",
            "Step: 253952\tEval reward: 6.10996150970459\n",
            "Step: 262144\tMean reward: 8.875\n",
            "Step: 262144\tEval reward: 6.505772113800049\n",
            "Step: 270336\tMean reward: 8.71875\n",
            "Step: 270336\tEval reward: 5.809121608734131\n",
            "Step: 278528\tMean reward: 9.78125\n",
            "Step: 278528\tEval reward: 6.509690761566162\n",
            "Step: 286720\tMean reward: 8.84375\n",
            "Step: 286720\tEval reward: 6.196836471557617\n",
            "Step: 294912\tMean reward: 9.59375\n",
            "Step: 294912\tEval reward: 5.486823081970215\n",
            "Step: 303104\tMean reward: 9.28125\n",
            "Step: 303104\tEval reward: 6.50945520401001\n",
            "Step: 311296\tMean reward: 8.21875\n",
            "Step: 311296\tEval reward: 5.798807621002197\n",
            "Step: 319488\tMean reward: 7.625\n",
            "Step: 319488\tEval reward: 5.258794784545898\n",
            "Step: 327680\tMean reward: 9.53125\n",
            "Step: 327680\tEval reward: 5.175857067108154\n",
            "Step: 335872\tMean reward: 9.5625\n",
            "Step: 335872\tEval reward: 6.330981254577637\n",
            "Step: 344064\tMean reward: 9.90625\n",
            "Step: 344064\tEval reward: 6.342129707336426\n",
            "Step: 352256\tMean reward: 9.21875\n",
            "Step: 352256\tEval reward: 6.260313510894775\n",
            "Step: 360448\tMean reward: 10.375\n",
            "Step: 360448\tEval reward: 5.971640110015869\n",
            "Step: 368640\tMean reward: 8.4375\n",
            "Step: 368640\tEval reward: 6.531429290771484\n",
            "Step: 376832\tMean reward: 11.40625\n",
            "Step: 376832\tEval reward: 5.873425006866455\n",
            "Step: 385024\tMean reward: 10.0625\n",
            "Step: 385024\tEval reward: 5.7086262702941895\n",
            "Step: 393216\tMean reward: 10.0625\n",
            "Step: 393216\tEval reward: 5.198334693908691\n",
            "Step: 401408\tMean reward: 8.8125\n",
            "Step: 401408\tEval reward: 5.754204750061035\n",
            "Step: 409600\tMean reward: 9.90625\n",
            "Step: 409600\tEval reward: 6.155162811279297\n",
            "Step: 417792\tMean reward: 8.96875\n",
            "Step: 417792\tEval reward: 6.009705543518066\n",
            "Step: 425984\tMean reward: 10.9375\n",
            "Step: 425984\tEval reward: 6.739348411560059\n",
            "Step: 434176\tMean reward: 8.34375\n",
            "Step: 434176\tEval reward: 5.721528053283691\n",
            "Step: 442368\tMean reward: 8.59375\n",
            "Step: 442368\tEval reward: 5.2299113273620605\n",
            "Step: 450560\tMean reward: 9.25\n",
            "Step: 450560\tEval reward: 5.682855129241943\n",
            "Step: 458752\tMean reward: 10.34375\n",
            "Step: 458752\tEval reward: 5.591249942779541\n",
            "Step: 466944\tMean reward: 9.96875\n",
            "Step: 466944\tEval reward: 6.0819091796875\n",
            "Step: 475136\tMean reward: 7.96875\n",
            "Step: 475136\tEval reward: 5.855167865753174\n",
            "Step: 483328\tMean reward: 8.8125\n",
            "Step: 483328\tEval reward: 5.3934783935546875\n",
            "Step: 491520\tMean reward: 8.875\n",
            "Step: 491520\tEval reward: 5.749216079711914\n",
            "Step: 499712\tMean reward: 9.09375\n",
            "Step: 499712\tEval reward: 6.017822742462158\n",
            "Step: 507904\tMean reward: 8.625\n",
            "Step: 507904\tEval reward: 7.1354265213012695\n",
            "Step: 516096\tMean reward: 8.15625\n",
            "Step: 516096\tEval reward: 5.719996452331543\n",
            "Step: 524288\tMean reward: 8.96875\n",
            "Step: 524288\tEval reward: 5.729286193847656\n",
            "Step: 532480\tMean reward: 10.65625\n",
            "Step: 532480\tEval reward: 5.943883895874023\n",
            "Step: 540672\tMean reward: 10.71875\n",
            "Step: 540672\tEval reward: 5.998399257659912\n",
            "Step: 548864\tMean reward: 8.78125\n",
            "Step: 548864\tEval reward: 6.0305070877075195\n",
            "Step: 557056\tMean reward: 9.4375\n",
            "Step: 557056\tEval reward: 5.918064594268799\n",
            "Step: 565248\tMean reward: 9.5625\n",
            "Step: 565248\tEval reward: 6.027009963989258\n",
            "Step: 573440\tMean reward: 10.09375\n",
            "Step: 573440\tEval reward: 5.81868839263916\n",
            "Step: 581632\tMean reward: 10.90625\n",
            "Step: 581632\tEval reward: 6.427187442779541\n",
            "Step: 589824\tMean reward: 8.96875\n",
            "Step: 589824\tEval reward: 5.807392120361328\n",
            "Step: 598016\tMean reward: 8.8125\n",
            "Step: 598016\tEval reward: 6.48959493637085\n",
            "Step: 606208\tMean reward: 10.75\n",
            "Step: 606208\tEval reward: 6.167699813842773\n",
            "Step: 614400\tMean reward: 12.34375\n",
            "Step: 614400\tEval reward: 5.686335563659668\n",
            "Step: 622592\tMean reward: 9.625\n",
            "Step: 622592\tEval reward: 6.22881555557251\n",
            "Step: 630784\tMean reward: 11.46875\n",
            "Step: 630784\tEval reward: 6.050570964813232\n",
            "Step: 638976\tMean reward: 10.6875\n",
            "Step: 638976\tEval reward: 6.3629231452941895\n",
            "Step: 647168\tMean reward: 10.59375\n",
            "Step: 647168\tEval reward: 6.627046585083008\n",
            "Step: 655360\tMean reward: 9.59375\n",
            "Step: 655360\tEval reward: 6.450438976287842\n",
            "Step: 663552\tMean reward: 8.46875\n",
            "Step: 663552\tEval reward: 6.1789960861206055\n",
            "Step: 671744\tMean reward: 11.1875\n",
            "Step: 671744\tEval reward: 6.757540702819824\n",
            "Step: 679936\tMean reward: 11.09375\n",
            "Step: 679936\tEval reward: 6.5613908767700195\n",
            "Step: 688128\tMean reward: 10.3125\n",
            "Step: 688128\tEval reward: 6.8310933113098145\n",
            "Step: 696320\tMean reward: 9.78125\n",
            "Step: 696320\tEval reward: 6.073033809661865\n",
            "Step: 704512\tMean reward: 10.625\n",
            "Step: 704512\tEval reward: 5.513169288635254\n",
            "Step: 712704\tMean reward: 10.625\n",
            "Step: 712704\tEval reward: 5.814239501953125\n",
            "Step: 720896\tMean reward: 10.71875\n",
            "Step: 720896\tEval reward: 6.815981864929199\n",
            "Step: 729088\tMean reward: 10.21875\n",
            "Step: 729088\tEval reward: 5.834163188934326\n",
            "Step: 737280\tMean reward: 11.3125\n",
            "Step: 737280\tEval reward: 5.921012878417969\n",
            "Step: 745472\tMean reward: 11.0625\n",
            "Step: 745472\tEval reward: 6.638320446014404\n",
            "Step: 753664\tMean reward: 10.25\n",
            "Step: 753664\tEval reward: 6.005661487579346\n",
            "Step: 761856\tMean reward: 11.40625\n",
            "Step: 761856\tEval reward: 6.562811851501465\n",
            "Step: 770048\tMean reward: 10.375\n",
            "Step: 770048\tEval reward: 6.593380451202393\n",
            "Step: 778240\tMean reward: 10.375\n",
            "Step: 778240\tEval reward: 5.84237813949585\n",
            "Step: 786432\tMean reward: 11.84375\n",
            "Step: 786432\tEval reward: 5.783358097076416\n",
            "Step: 794624\tMean reward: 12.40625\n",
            "Step: 794624\tEval reward: 6.327329158782959\n",
            "Step: 802816\tMean reward: 11.5\n",
            "Step: 802816\tEval reward: 6.865818977355957\n",
            "Step: 811008\tMean reward: 11.125\n",
            "Step: 811008\tEval reward: 7.086704730987549\n",
            "Step: 819200\tMean reward: 11.5625\n",
            "Step: 819200\tEval reward: 6.323943614959717\n",
            "Step: 827392\tMean reward: 10.25\n",
            "Step: 827392\tEval reward: 6.035676002502441\n",
            "Step: 835584\tMean reward: 10.53125\n",
            "Step: 835584\tEval reward: 6.2776336669921875\n",
            "Step: 843776\tMean reward: 10.1875\n",
            "Step: 843776\tEval reward: 5.145817756652832\n",
            "Step: 851968\tMean reward: 9.90625\n",
            "Step: 851968\tEval reward: 5.815550804138184\n",
            "Step: 860160\tMean reward: 10.96875\n",
            "Step: 860160\tEval reward: 6.401638507843018\n",
            "Step: 868352\tMean reward: 10.8125\n",
            "Step: 868352\tEval reward: 5.484512805938721\n",
            "Step: 876544\tMean reward: 10.71875\n",
            "Step: 876544\tEval reward: 5.555532932281494\n",
            "Step: 884736\tMean reward: 11.03125\n",
            "Step: 884736\tEval reward: 6.222471714019775\n",
            "Step: 892928\tMean reward: 13.15625\n",
            "Step: 892928\tEval reward: 6.472708702087402\n",
            "Step: 901120\tMean reward: 12.21875\n",
            "Step: 901120\tEval reward: 6.078128337860107\n",
            "Step: 909312\tMean reward: 11.53125\n",
            "Step: 909312\tEval reward: 5.705906867980957\n",
            "Step: 917504\tMean reward: 11.03125\n",
            "Step: 917504\tEval reward: 6.205003261566162\n",
            "Step: 925696\tMean reward: 12.5\n",
            "Step: 925696\tEval reward: 6.115208625793457\n",
            "Step: 933888\tMean reward: 12.375\n",
            "Step: 933888\tEval reward: 5.7664337158203125\n",
            "Step: 942080\tMean reward: 13.03125\n",
            "Step: 942080\tEval reward: 6.1490559577941895\n",
            "Step: 950272\tMean reward: 12.53125\n",
            "Step: 950272\tEval reward: 5.958693981170654\n",
            "Step: 958464\tMean reward: 12.125\n",
            "Step: 958464\tEval reward: 5.281789302825928\n",
            "Step: 966656\tMean reward: 12.1875\n",
            "Step: 966656\tEval reward: 6.375322341918945\n",
            "Step: 974848\tMean reward: 10.25\n",
            "Step: 974848\tEval reward: 5.885469436645508\n",
            "Step: 983040\tMean reward: 11.96875\n",
            "Step: 983040\tEval reward: 5.387871742248535\n",
            "Step: 991232\tMean reward: 12.8125\n",
            "Step: 991232\tEval reward: 5.590387344360352\n",
            "Step: 999424\tMean reward: 10.78125\n",
            "Step: 999424\tEval reward: 6.776677131652832\n",
            "Step: 1007616\tMean reward: 12.28125\n",
            "Step: 1007616\tEval reward: 5.798745632171631\n",
            "Step: 1015808\tMean reward: 11.40625\n",
            "Step: 1015808\tEval reward: 6.081817150115967\n",
            "Step: 1024000\tMean reward: 12.0625\n",
            "Step: 1024000\tEval reward: 6.9410576820373535\n",
            "Step: 1032192\tMean reward: 11.125\n",
            "Step: 1032192\tEval reward: 6.288480281829834\n",
            "Step: 1040384\tMean reward: 12.125\n",
            "Step: 1040384\tEval reward: 5.926675796508789\n",
            "Step: 1048576\tMean reward: 12.59375\n",
            "Step: 1048576\tEval reward: 6.322563171386719\n",
            "Step: 1056768\tMean reward: 11.59375\n",
            "Step: 1056768\tEval reward: 5.452477931976318\n",
            "Step: 1064960\tMean reward: 11.90625\n",
            "Step: 1064960\tEval reward: 5.408926486968994\n",
            "Step: 1073152\tMean reward: 11.34375\n",
            "Step: 1073152\tEval reward: 6.239827632904053\n",
            "Step: 1081344\tMean reward: 11.65625\n",
            "Step: 1081344\tEval reward: 6.224348068237305\n",
            "Step: 1089536\tMean reward: 11.46875\n",
            "Step: 1089536\tEval reward: 6.029325485229492\n",
            "Step: 1097728\tMean reward: 11.125\n",
            "Step: 1097728\tEval reward: 6.234527111053467\n",
            "Step: 1105920\tMean reward: 12.21875\n",
            "Step: 1105920\tEval reward: 6.278850078582764\n",
            "Step: 1114112\tMean reward: 10.0\n",
            "Step: 1114112\tEval reward: 6.413010597229004\n",
            "Step: 1122304\tMean reward: 11.15625\n",
            "Step: 1122304\tEval reward: 5.256776332855225\n",
            "Step: 1130496\tMean reward: 11.34375\n",
            "Step: 1130496\tEval reward: 5.974597454071045\n",
            "Step: 1138688\tMean reward: 12.8125\n",
            "Step: 1138688\tEval reward: 6.531155109405518\n",
            "Step: 1146880\tMean reward: 11.84375\n",
            "Step: 1146880\tEval reward: 6.800154209136963\n",
            "Step: 1155072\tMean reward: 10.25\n",
            "Step: 1155072\tEval reward: 5.9423828125\n",
            "Step: 1163264\tMean reward: 11.53125\n",
            "Step: 1163264\tEval reward: 5.811758518218994\n",
            "Step: 1171456\tMean reward: 12.75\n",
            "Step: 1171456\tEval reward: 6.08734130859375\n",
            "Step: 1179648\tMean reward: 11.96875\n",
            "Step: 1179648\tEval reward: 5.889547348022461\n",
            "Step: 1187840\tMean reward: 13.53125\n",
            "Step: 1187840\tEval reward: 6.353804588317871\n",
            "Step: 1196032\tMean reward: 13.65625\n",
            "Step: 1196032\tEval reward: 6.274870872497559\n",
            "Step: 1204224\tMean reward: 12.40625\n",
            "Step: 1204224\tEval reward: 6.230238437652588\n",
            "Step: 1212416\tMean reward: 10.21875\n",
            "Step: 1212416\tEval reward: 6.568118095397949\n",
            "Step: 1220608\tMean reward: 12.0\n",
            "Step: 1220608\tEval reward: 5.508544445037842\n",
            "Step: 1228800\tMean reward: 13.9375\n",
            "Step: 1228800\tEval reward: 5.767580986022949\n",
            "Step: 1236992\tMean reward: 12.21875\n",
            "Step: 1236992\tEval reward: 6.17647123336792\n",
            "Step: 1245184\tMean reward: 12.125\n",
            "Step: 1245184\tEval reward: 6.37454891204834\n",
            "Step: 1253376\tMean reward: 13.1875\n",
            "Step: 1253376\tEval reward: 6.1701579093933105\n",
            "Step: 1261568\tMean reward: 12.0625\n",
            "Step: 1261568\tEval reward: 5.198192596435547\n",
            "Step: 1269760\tMean reward: 12.78125\n",
            "Step: 1269760\tEval reward: 5.540517807006836\n",
            "Step: 1277952\tMean reward: 12.125\n",
            "Step: 1277952\tEval reward: 5.435058116912842\n",
            "Step: 1286144\tMean reward: 13.5625\n",
            "Step: 1286144\tEval reward: 5.960152626037598\n",
            "Step: 1294336\tMean reward: 12.6875\n",
            "Step: 1294336\tEval reward: 6.635533809661865\n",
            "Step: 1302528\tMean reward: 11.53125\n",
            "Step: 1302528\tEval reward: 5.567278861999512\n",
            "Step: 1310720\tMean reward: 12.09375\n",
            "Step: 1310720\tEval reward: 5.4435038566589355\n",
            "Step: 1318912\tMean reward: 11.0625\n",
            "Step: 1318912\tEval reward: 5.407358646392822\n",
            "Step: 1327104\tMean reward: 10.4375\n",
            "Step: 1327104\tEval reward: 6.560631275177002\n",
            "Step: 1335296\tMean reward: 11.625\n",
            "Step: 1335296\tEval reward: 4.918025970458984\n",
            "Step: 1343488\tMean reward: 11.6875\n",
            "Step: 1343488\tEval reward: 5.9928388595581055\n",
            "Step: 1351680\tMean reward: 10.5\n",
            "Step: 1351680\tEval reward: 6.051810264587402\n",
            "Step: 1359872\tMean reward: 11.46875\n",
            "Step: 1359872\tEval reward: 5.650007247924805\n",
            "Step: 1368064\tMean reward: 10.5\n",
            "Step: 1368064\tEval reward: 5.6163225173950195\n",
            "Step: 1376256\tMean reward: 11.21875\n",
            "Step: 1376256\tEval reward: 5.766974449157715\n",
            "Step: 1384448\tMean reward: 11.28125\n",
            "Step: 1384448\tEval reward: 6.017610549926758\n",
            "Step: 1392640\tMean reward: 12.1875\n",
            "Step: 1392640\tEval reward: 6.096157550811768\n",
            "Step: 1400832\tMean reward: 10.4375\n",
            "Step: 1400832\tEval reward: 6.365260601043701\n",
            "Step: 1409024\tMean reward: 10.375\n",
            "Step: 1409024\tEval reward: 5.9355363845825195\n",
            "Step: 1417216\tMean reward: 11.46875\n",
            "Step: 1417216\tEval reward: 5.779181480407715\n",
            "Step: 1425408\tMean reward: 12.8125\n",
            "Step: 1425408\tEval reward: 5.2880988121032715\n",
            "Step: 1433600\tMean reward: 10.71875\n",
            "Step: 1433600\tEval reward: 5.2050371170043945\n",
            "Step: 1441792\tMean reward: 10.96875\n",
            "Step: 1441792\tEval reward: 5.578364849090576\n",
            "Step: 1449984\tMean reward: 10.8125\n",
            "Step: 1449984\tEval reward: 5.337583541870117\n",
            "Step: 1458176\tMean reward: 11.125\n",
            "Step: 1458176\tEval reward: 5.744982719421387\n",
            "Step: 1466368\tMean reward: 11.53125\n",
            "Step: 1466368\tEval reward: 5.793742656707764\n",
            "Step: 1474560\tMean reward: 11.53125\n",
            "Step: 1474560\tEval reward: 5.925551891326904\n",
            "Step: 1482752\tMean reward: 11.40625\n",
            "Step: 1482752\tEval reward: 5.66812801361084\n",
            "Step: 1490944\tMean reward: 11.0\n",
            "Step: 1490944\tEval reward: 5.516410827636719\n",
            "Step: 1499136\tMean reward: 10.28125\n",
            "Step: 1499136\tEval reward: 5.16069221496582\n",
            "Step: 1507328\tMean reward: 12.40625\n",
            "Step: 1507328\tEval reward: 5.292336463928223\n",
            "Step: 1515520\tMean reward: 10.96875\n",
            "Step: 1515520\tEval reward: 6.051069259643555\n",
            "Step: 1523712\tMean reward: 13.15625\n",
            "Step: 1523712\tEval reward: 5.825199604034424\n",
            "Step: 1531904\tMean reward: 10.3125\n",
            "Step: 1531904\tEval reward: 5.654659271240234\n",
            "Step: 1540096\tMean reward: 11.4375\n",
            "Step: 1540096\tEval reward: 5.722390174865723\n",
            "Step: 1548288\tMean reward: 12.375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAZrWuVGLTu-"
      },
      "source": [
        "Below cell can be used for policy evaluation and saves an episode to mp4 for you to view."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zecOCkd7Jzt"
      },
      "source": [
        "# print(type(total_reward))\n",
        "# import pickle\n",
        "# with open('storage1.pkl', 'wb') as f:\n",
        "#     pickle.dump(storage, f)\n",
        "# import imageio\n",
        "\n",
        "# # Make evaluation environment\n",
        "# eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n",
        "# obs = eval_env.reset()\n",
        "\n",
        "# frames = []\n",
        "# total_reward = []\n",
        "\n",
        "# # Evaluate policy\n",
        "# policy.eval()#512\n",
        "# for _ in range(512):\n",
        "\n",
        "#   # Use policy\n",
        "#   action, log_prob, value = policy.act(obs)\n",
        "\n",
        "#   # Take step in environment\n",
        "#   obs, reward, done, info = eval_env.step(action)\n",
        "#   total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "#   # Render environment and store\n",
        "#   frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "#   frames.append(frame)\n",
        "\n",
        "# # Calculate average return\n",
        "# total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "# print('Average return:', total_reward)\n",
        "\n",
        "\n",
        "\n",
        "# # Save frames as video\n",
        "# frames = torch.stack(frames)\n",
        "# imageio.mimsave('cropvid.mp4', frames, fps=25)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37qB1OWewf4R"
      },
      "source": [
        "# print(type(storage))\n",
        "\n",
        "# from google.colab import files\n",
        "# files.download('cropvid.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGPMzsZCp5xO"
      },
      "source": [
        "# # !pip install kora\n",
        "# # from kora import console\n",
        "# # console.start()\n",
        "# !pip install procgen\n",
        "# !wget https://raw.githubusercontent.com/nicklashansen/ppo-procgen-utils/main/utils.py\n",
        "# !wget https://raw.githubusercontent.com/MishaLaskin/rad/1246bfd6e716669126e12c1f02f393801e1692c1/TransformLayer.py\n",
        "# # Hyperparameters\n",
        "# total_steps = 8e6\n",
        "# num_envs = 32\n",
        "# num_levels = 100\n",
        "# num_steps = 256\n",
        "# num_epochs = 3\n",
        "# batch_size = 512 #512\n",
        "# eps = .2\n",
        "# grad_eps = .5\n",
        "# value_coef = .5\n",
        "# entropy_coef = .01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saDiq53q2TZl"
      },
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from utils import make_env, Storage, orthogonal_init\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# from utils import make_env, Storage, orthogonal_init\n",
        "  \n",
        "\n",
        "# class Flatten(nn.Module):\n",
        "#     def forward(self, x):\n",
        "#         return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "# class Encoder(nn.Module):\n",
        "#   def __init__(self, in_channels, feature_dim):\n",
        "#     super().__init__()\n",
        "#     self.layers = nn.Sequential(\n",
        "#         nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=8, stride=4), nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2), nn.ReLU(),\n",
        "#         nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1), nn.ReLU(),\n",
        "#         Flatten(),\n",
        "#         nn.Linear(in_features=1024, out_features=feature_dim), nn.ReLU()\n",
        "#     )\n",
        "#     self.apply(orthogonal_init)\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     return self.layers(x)\n",
        "\n",
        "\n",
        "# class Policy(nn.Module):\n",
        "#   def __init__(self, encoder, feature_dim, num_actions):\n",
        "#     super().__init__()\n",
        "#     self.encoder = encoder\n",
        "#     self.policy = orthogonal_init(nn.Linear(feature_dim, num_actions), gain=.01)\n",
        "#     self.value = orthogonal_init(nn.Linear(feature_dim, 1), gain=1.)\n",
        "\n",
        "#   def act(self, x):\n",
        "#     with torch.no_grad():\n",
        "#       x = x.cuda().contiguous()\n",
        "#       dist, value = self.forward(x)\n",
        "#       action = dist.sample()\n",
        "#       log_prob = dist.log_prob(action)\n",
        "    \n",
        "#     return action.cpu(), log_prob.cpu(), value.cpu()\n",
        "\n",
        "#   def forward(self, x):\n",
        "#     x = self.encoder(x)\n",
        "#     logits = self.policy(x)\n",
        "#     value = self.value(x).squeeze(1)\n",
        "#     dist = torch.distributions.Categorical(logits=logits)\n",
        "\n",
        "#     return dist, value\n",
        "\n",
        "\n",
        "# # Define environmentbossfight\n",
        "# # check the utils.py file for info on arguments\n",
        "# eval_env = make_env(num_envs, env_name = 'starpilot',start_level=num_levels, num_levels=num_levels, use_backgrounds=True)\n",
        "# obs = eval_env.reset()\n",
        "# total_reward = []\n",
        "\n",
        "# # Define network\n",
        "# encoder = Encoder(3,512)\n",
        "# policy = Policy(encoder, 512, 15)\n",
        "# policy.cuda()\n",
        "# policy.load_state_dict(torch.load('checkpoint.pt'))\n",
        "# policy.eval()\n",
        "\n",
        "\n",
        "\n",
        "# frames = []\n",
        "# total_reward = []\n",
        "\n",
        "# # Evaluate policy\n",
        "# policy.eval()#512\n",
        "# for _ in range(512):\n",
        "\n",
        "#   # Use policy\n",
        "#   action, log_prob, value = policy.act(obs)\n",
        "\n",
        "#   # Take step in environment\n",
        "#   obs, reward, done, info = eval_env.step(action)\n",
        "#   total_reward.append(torch.Tensor(reward))\n",
        "\n",
        "#   # Render environment and store\n",
        "#   frame = (torch.Tensor(eval_env.render(mode='rgb_array'))*255.).byte()\n",
        "#   frames.append(frame)\n",
        "\n",
        "# # Calculate average return\n",
        "# # total_reward = torch.stack(total_reward).sum(0).mean(0)\n",
        "# # print('Average return:', total_reward)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7kjOfWm48RO"
      },
      "source": [
        "# print(len(total_reward))\n",
        "# print(len(total_reward[0]))\n",
        "# ree=0.0\n",
        "# reet=[]\n",
        "# for i in range(len(total_reward)):\n",
        "#     ree+=total_reward[i].mean()\n",
        "#     reet.append(total_reward[i].mean())\n",
        "# print(ree)\n",
        "# print(reet)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}